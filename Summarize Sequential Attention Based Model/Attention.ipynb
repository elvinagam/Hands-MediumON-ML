{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from official.nlp.modeling import layers\n",
    "\n",
    "\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "  \"\"\"Multi-headed attention layer.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_size, num_heads, attention_dropout):\n",
    "    \"\"\"Initialize Attention.\n",
    "    Args:\n",
    "      hidden_size: int, output dim of hidden layer.\n",
    "      num_heads: int, number of heads to repeat the same attention structure.\n",
    "      attention_dropout: float, dropout rate inside attention for training.\n",
    "    \"\"\"\n",
    "    if hidden_size % num_heads:\n",
    "      raise ValueError(\n",
    "          \"Hidden size ({}) must be divisible by the number of heads ({}).\"\n",
    "          .format(hidden_size, num_heads))\n",
    "\n",
    "    super(Attention, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_heads = num_heads\n",
    "    self.attention_dropout = attention_dropout\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    \"\"\"Builds the layer.\"\"\"\n",
    "    # Layers for linearly projecting the queries, keys, and values.\n",
    "    size_per_head = self.hidden_size // self.num_heads\n",
    "\n",
    "    def _glorot_initializer(fan_in, fan_out):\n",
    "      limit = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "      return tf.keras.initializers.RandomUniform(minval=-limit, maxval=limit)\n",
    "\n",
    "    attention_initializer = _glorot_initializer(input_shape.as_list()[-1],\n",
    "                                                self.hidden_size)\n",
    "    self.query_dense_layer = layers.DenseEinsum(\n",
    "        output_shape=(self.num_heads, size_per_head),\n",
    "        kernel_initializer=attention_initializer,\n",
    "        use_bias=False,\n",
    "        name=\"query\")\n",
    "    self.key_dense_layer = layers.DenseEinsum(\n",
    "        output_shape=(self.num_heads, size_per_head),\n",
    "        kernel_initializer=attention_initializer,\n",
    "        use_bias=False,\n",
    "        name=\"key\")\n",
    "    self.value_dense_layer = layers.DenseEinsum(\n",
    "        output_shape=(self.num_heads, size_per_head),\n",
    "        kernel_initializer=attention_initializer,\n",
    "        use_bias=False,\n",
    "        name=\"value\")\n",
    "\n",
    "    output_initializer = _glorot_initializer(self.hidden_size, self.hidden_size)\n",
    "    self.output_dense_layer = layers.DenseEinsum(\n",
    "        output_shape=self.hidden_size,\n",
    "        num_summed_dimensions=2,\n",
    "        kernel_initializer=output_initializer,\n",
    "        use_bias=False,\n",
    "        name=\"output_transform\")\n",
    "    super(Attention, self).build(input_shape)\n",
    "\n",
    "  def get_config(self):\n",
    "    return {\n",
    "        \"hidden_size\": self.hidden_size,\n",
    "        \"num_heads\": self.num_heads,\n",
    "        \"attention_dropout\": self.attention_dropout,\n",
    "    }\n",
    "\n",
    "  def call(self,\n",
    "           query_input,\n",
    "           source_input,\n",
    "           bias,\n",
    "           training,\n",
    "           cache=None,\n",
    "           decode_loop_step=None):\n",
    "    \"\"\"Apply attention mechanism to query_input and source_input.\n",
    "    Args:\n",
    "      query_input: A tensor with shape [batch_size, length_query, hidden_size].\n",
    "      source_input: A tensor with shape [batch_size, length_source,\n",
    "        hidden_size].\n",
    "      bias: A tensor with shape [batch_size, 1, length_query, length_source],\n",
    "        the attention bias that will be added to the result of the dot product.\n",
    "      training: A bool, whether in training mode or not.\n",
    "      cache: (Used during prediction) A dictionary with tensors containing\n",
    "        results of previous attentions. The dictionary must have the items:\n",
    "            {\"k\": tensor with shape [batch_size, i, heads, dim_per_head],\n",
    "             \"v\": tensor with shape [batch_size, i, heads, dim_per_head]} where\n",
    "               i is the current decoded length for non-padded decode, or max\n",
    "               sequence length for padded decode.\n",
    "      decode_loop_step: An integer, step number of the decoding loop. Used only\n",
    "        for autoregressive inference on TPU.\n",
    "    Returns:\n",
    "      Attention layer output with shape [batch_size, length_query, hidden_size]\n",
    "    \"\"\"\n",
    "    # Linearly project the query, key and value using different learned\n",
    "    # projections. Splitting heads is automatically done during the linear\n",
    "    # projections --> [batch_size, length, num_heads, dim_per_head].\n",
    "    query = self.query_dense_layer(query_input)\n",
    "    key = self.key_dense_layer(source_input)\n",
    "    value = self.value_dense_layer(source_input)\n",
    "\n",
    "    if cache is not None:\n",
    "      # Combine cached keys and values with new keys and values.\n",
    "      if decode_loop_step is not None:\n",
    "        cache_k_shape = cache[\"k\"].shape.as_list()\n",
    "        indices = tf.reshape(\n",
    "            tf.one_hot(decode_loop_step, cache_k_shape[1], dtype=key.dtype),\n",
    "            [1, cache_k_shape[1], 1, 1])\n",
    "        key = cache[\"k\"] + key * indices\n",
    "        cache_v_shape = cache[\"v\"].shape.as_list()\n",
    "        indices = tf.reshape(\n",
    "            tf.one_hot(decode_loop_step, cache_v_shape[1], dtype=value.dtype),\n",
    "            [1, cache_v_shape[1], 1, 1])\n",
    "        value = cache[\"v\"] + value * indices\n",
    "      else:\n",
    "        key = tf.concat([tf.cast(cache[\"k\"], key.dtype), key], axis=1)\n",
    "        value = tf.concat([tf.cast(cache[\"v\"], value.dtype), value], axis=1)\n",
    "\n",
    "      # Update cache\n",
    "      cache[\"k\"] = key\n",
    "      cache[\"v\"] = value\n",
    "\n",
    "    # Scale query to prevent the dot product between query and key from growing\n",
    "    # too large.\n",
    "    depth = (self.hidden_size // self.num_heads)\n",
    "    query *= depth**-0.5\n",
    "\n",
    "    # Calculate dot product attention\n",
    "    logits = tf.einsum(\"BTNH,BFNH->BNFT\", key, query)\n",
    "    logits += bias\n",
    "    # Note that softmax internally performs math operations using float32\n",
    "    # for numeric stability. When training with float16, we keep the input\n",
    "    # and output in float16 for better performance.\n",
    "    weights = tf.nn.softmax(logits, name=\"attention_weights\")\n",
    "    if training:\n",
    "      weights = tf.nn.dropout(weights, rate=self.attention_dropout)\n",
    "    attention_output = tf.einsum(\"BNFT,BTNH->BFNH\", weights, value)\n",
    "\n",
    "    # Run the outputs through another linear projection layer. Recombining heads\n",
    "    # is automatically done --> [batch_size, length, hidden_size]\n",
    "    attention_output = self.output_dense_layer(attention_output)\n",
    "    return attention_output\n",
    "\n",
    "\n",
    "class SelfAttention(Attention):\n",
    "  \"\"\"Multiheaded self-attention layer.\"\"\"\n",
    "\n",
    "  def call(self,\n",
    "           query_input,\n",
    "           bias,\n",
    "           training,\n",
    "           cache=None,\n",
    "           decode_loop_step=None):\n",
    "    return super(SelfAttention, self).call(query_input, query_input, bias,\n",
    "                                           training, cache, decode_loop_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
